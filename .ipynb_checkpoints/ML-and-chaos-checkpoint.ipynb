{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Chaotic attractors\n",
    "\n",
    "Here we try to reproduce some [results](https://arxiv.org/pdf/1710.07313.pdf) by Pothak et al. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $A$-matrix generation\n",
    "\n",
    "Due to the paper the $A$ matrix represents the adjacency matrix of so-called sparse random Erdos–R􏰐enyi graph. There are three parameters that define it: $N$, $D$ and $\\rho$. $N$ is just a size of matrix. $D$ is an average degree of a vertex. And the non-zero elements are taken to be uniformly distributed on the interval $(-a, a)$ whre $a$ is chosen in a such way that the maximum eigenvalue of $A$ is equal to $\\rho$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import random as randm\n",
    "from scipy import stats\n",
    "from numpy.linalg import eigvals\n",
    "\n",
    "def generate_A(N, D, rho):\n",
    "    loc = -1\n",
    "    scale = abs(loc*2)\n",
    "    rvs = stats.uniform(loc = loc, scale = scale).rvs\n",
    "    \n",
    "    A = randm(N, N, density=D/N, data_rvs = rvs)\n",
    "    A = np.tril(A.A) + np.tril(A.A, -1).T #to keep distribution and density right\n",
    "    rescale = eigvals(A).max()/rho\n",
    "    A = A/rescale\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $W_{in}$ generation\n",
    "\n",
    "$W_{in}$ helps to transform low-dimensional input vector $u$ to $N$-dimensional vector. So, $W_{in}$ is a $N \\times D_{in}$ matrix, where $D_{in}$ is the dimension of $u$. $W_{in}$ looks like vertical stack of diagonal $D_{in} \\times D_{in}$ matrix, where diagonal elements are uniformly random in range $(-\\sigma, \\sigma)$.  \n",
    "\n",
    "According to the paper the result of learning should not substentionally depend on $W_{in}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_W_in(N, D_input, sigma):\n",
    "    dim = N//D_input #should be integer by design!\n",
    "    \n",
    "    W_in = np.zeros((D_input, D_input))\n",
    "    for i in range(D_input):\n",
    "        W_in[i][i] = random.uniform(-sigma, sigma)\n",
    "        \n",
    "    for j in range(dim-1):\n",
    "        W_part = np.zeros((D_input, D_input))\n",
    "        for i in range(D_input):\n",
    "            W_part[i][i] = random.uniform(-sigma, sigma)\n",
    "        W_in = np.vstack((W_in, W_part))\n",
    "        \n",
    "    return W_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
